data:
  atomic_number:
  - 3
  - 15
  - 16
  r_cut: 6.0
  test_batch_size: 4
  testset_filename: /project/wen/mjwen/playground/natip_playground/dataset/LiPS/20240112/test_LiPS.json
  train_batch_size: 2
  trainset_filename: /project/wen/mjwen/playground/natip_playground/dataset/LiPS/20240112/train_10_LiPS.json
  val_batch_size: 20
  valset_filename: /project/wen/mjwen/playground/natip_playground/dataset/LiPS/20240112/val_LiPS.json
ema:
  beta: 0.999
  include_online_model: false
  power: 0.6667
  update_after_step: 1000
  update_every: 1
loss:
  energy_ratio: 1.0
  forces_ratio: 1.0
  normalize: true
lr_scheduler:
  class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
  init_args:
    factor: 0.8
    mode: min
    patience: 100
    verbose: true
  monitor: val/mae_f
metrics:
  normalize: true
  type: mae
model:
  max_chebyshev_degree: 8
  max_u: 16
  max_v: 3
  num_average_neigh: auto
  num_layers: 2
  output_mlp_hidden_layers:
  - 16
  - 16
  radial_mlp_hidden_layers:
  - 16
  - 16
optimizer:
  class_path: torch.optim.Adam
  init_args:
    amsgrad: true
    lr: 0.01
    weight_decay: 0.0
restore_checkpoint: null
seed_everything: 35
trainer:
  accelerator: gpu
  callbacks:
  - class_path: lightning.pytorch.callbacks.ModelSummary
    init_args:
      max_depth: -1
  - class_path: lightning.pytorch.callbacks.LearningRateMonitor
    init_args:
      logging_interval: null
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      mode: min
      monitor: val_ema/mae_e
      save_top_k: 3
      verbose: false
  - class_path: lightning.pytorch.callbacks.EarlyStopping
    init_args:
      mode: min
      monitor: val_ema/mae_e
      patience: 400
      verbose: true
  check_val_every_n_epoch: 1
  detect_anomaly: false
  gradient_clip_val: 100.0
  inference_mode: false
  log_every_n_steps: 50
  logger:
    class_path: lightning.pytorch.loggers.wandb.WandbLogger
    init_args:
      project: 240509_LiPS_10
  max_epochs: 5000
  num_nodes: 1
