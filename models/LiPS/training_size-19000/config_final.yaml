data:
  atomic_number:
  - 3
  - 15
  - 16
  r_cut: 6.0
  test_batch_size: 4
  testset_filename: /project/wen/mjwen/playground/natip_playground/dataset/LiPS/20240112/test_LiPS.json
  train_batch_size: 8
  trainset_filename: /project/wen/mjwen/playground/natip_playground/dataset/LiPS/20240112/train_19000_LiPS.json
  val_batch_size: 12
  valset_filename: /project/wen/mjwen/playground/natip_playground/dataset/LiPS/20240112/val_LiPS.json
default_dtype: float32
ema:
  beta: 0.999
  include_online_model: false
  power: 0.6667
  update_after_step: 1000
  update_every: 1
loss:
  energy_ratio: 1.0
  forces_ratio: 1.0
  normalize: true
lr_scheduler:
  class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
  init_args:
    factor: 0.8
    mode: min
    patience: 100
    verbose: true
  monitor: val/mae_f
metrics:
  normalize: true
  type: mae
model:
  max_chebyshev_degree: 8
  max_u: 48
  max_v: 3
  num_average_neigh: auto
  num_layers: 3
  output_mlp_hidden_layers:
  - 48
  - 48
  radial_mlp_hidden_layers:
  - 48
  - 48
optimizer:
  class_path: torch.optim.Adam
  init_args:
    amsgrad: true
    lr: 0.01
    weight_decay: 0.0
restore_checkpoint: /project/wen/mjwen/playground/natip_playground/LiPS/20240803/job_dir-19000/job_8/240803_LiPS_19000/gefq69zg/checkpoints/epoch=2432-step=5778375.ckpt
seed_everything: 4
trainer:
  accelerator: gpu
  callbacks:
  - class_path: lightning.pytorch.callbacks.ModelSummary
    init_args:
      max_depth: -1
  - class_path: lightning.pytorch.callbacks.LearningRateMonitor
    init_args:
      logging_interval: null
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      mode: min
      monitor: val_ema/mae_e
      save_top_k: 3
      verbose: false
  - class_path: lightning.pytorch.callbacks.EarlyStopping
    init_args:
      mode: min
      monitor: val_ema/mae_e
      patience: 200
      verbose: true
  check_val_every_n_epoch: 1
  detect_anomaly: false
  gradient_clip_val: 100.0
  inference_mode: false
  log_every_n_steps: 50
  logger:
    class_path: lightning.pytorch.loggers.wandb.WandbLogger
    init_args:
      project: 240803_LiPS_19000
  max_epochs: 3000
  num_nodes: 1
